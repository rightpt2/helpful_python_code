
## this is a nifty little function that lets me return a True or False if a string contains another string
rct_summary['Tecate']= rct_summary.Brand.str.contains('Tecate')

## this is a helpful link to some other good snipits to borrow from
https://gist.github.com/bsweger/e5817488d161f37dcbd2


## delete a column from a data frame
del df['column']


## renaming columns
df.rename(columns={'orginal_name':'new_name', 'col2':'col2_new'}, inplace=true)


## selecting a subset from a data frame - Data isin

include_set = ['CVS', 'Target', 'Walgreens']
df2 = df1[df1['Chain Group'].isin(include_set)]

## selecting a subset from a data frame - Data isin


df2 = df1[df1['Chain Group'] = 'paramter' ]


## select a subset from a data frame - Data is NOT in

include_set = ['CVS', 'Target', 'Walgreens']
df2 = df1[~df1['Chain Group'].isin(include_set)]


## Merging tables is a great way to pull data together efficiently

df3 = pd.merge(df1, df2, how='type', left_on='columns', right_on='columns')

## SQL Alchemy is a rad way to pull in data
store_layers = pd.read_sql("SELECT * \
                            FROM subscription_layers \
                            WHERE customer_id = {0} \
                            AND type = 'stores_layer'".format(customer_id), postgres_engine) 

# Creating the engine is pretty easy- here you add the string for the access to your database 
engine = create_engine('sqlite:///Chinook.sqlite') 

# Another way to do this is create a connection and close it with a with. (Perform query and save results to DataFrame: df)
with engine.connect() as con:
    rs = con.execute("SELECT * FROM Employee WHERE EmployeeId >= 6")
    df = pd.DataFrame(rs.fetchall())
    df.columns = rs.keys()





## Here I would like to add some interesting statistics stuff I might need in the future
## numpy is great for having lots of these bulit in
np.random.seed(integer)
np.random.rand()



# Importing files can be rad
# excel to a data frame
filename = 'file.xlsx'
data = pd.ExcelFile(filename)

# now we can pull just the sheet names
data.sheet_names

## the next fun thing to do is parse a sheet into a new data frame
df1 = data.parse(sheetname)  [you can alse you index]

#another need thing to use is some parts of the parse function, be sure to use the brackes for a list
df1 = data.parse(0, skiprows= [0], names=['Country','AAM due to War (2002)'])


## This one is a great one. It allows you to group by in a data frame then join the strings together from a field that is not in the group by
# Sexy!
df2 = df1.groupby(['job_id','store_id','Pepsi_Group'])['display_present_name'].apply(','.join)
df2 = df2.to_frame()
df2 = df2.reset_index()




## You can filter on more than one criteria at a time
working = simple_table[(simple_table['Store_id'] == 28341) & (simple_table['brands_coolers'] == 'Misc.')]

## This is a helpful one for queries, you can join all of the values from a column in a pandas data frame to a text string separated by a comma
list = ','.join([str(id) for id in df['column'].tolist()])

## this lets you combine a multi level index into one level - very helpful after stacking or unstacking data

df.columns = [' '.join(col).strip() for col in df.columns.values]



## This is a great way to create an excel workbook with many worksheets
writer = pd.ExcelWriter( Wave + '_Weekly_output' + today + '.xlsx')
answers_responses_brands.to_excel(writer,'full_answers_responses')
test_group_OSA.to_excel(writer,'OSA_this_Week')
writer.save()


##urllib is a cool library to get info from a website. Python 3.0 often pulls it as a bit so here is an easier decoder for text
with urllib.request.urlopen('http://www.crummy.com/software/BeautifulSoup') as response:
    raw_source = response.read()
    source = raw_source.decode("utf-8")


#Determine pivot table
impute_grps = data.pivot_table(values=["LoanAmount"], index=["Gender","Married","Self_Employed"], aggfunc=np.mean)
print impute_grps


# for data cleaning, converts str into numbers, and sets errors to NAN
pd.to_numeric([array],errors='coerce') 

